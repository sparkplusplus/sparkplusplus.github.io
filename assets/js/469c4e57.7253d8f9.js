"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[498],{3905:(e,t,n)=>{n.d(t,{Zo:()=>s,kt:()=>u});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function p(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),d=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},s=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},m="mdxType",k={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,s=p(e,["components","mdxType","originalType","parentName"]),m=d(n),c=i,u=m["".concat(l,".").concat(c)]||m[c]||k[c]||r;return n?a.createElement(u,o(o({ref:t},s),{},{components:n})):a.createElement(u,o({ref:t},s))}));function u(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=c;var p={};for(var l in t)hasOwnProperty.call(t,l)&&(p[l]=t[l]);p.originalType=e,p[m]="string"==typeof e?e:i,o[1]=p;for(var d=2;d<r;d++)o[d]=n[d];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},1313:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>k,frontMatter:()=>r,metadata:()=>p,toc:()=>d});var a=n(7462),i=(n(7294),n(3905));const r={title:"Spark Logical Plan",sidebar_position:2,id:"spark-logical-plan",description:"Prophecy deployment is flexible and supports multiple mechanisms",tags:["overview","spark-internals"]},o=void 0,p={unversionedId:"spark-internals/plans/spark-logical-plan",id:"spark-internals/plans/spark-logical-plan",title:"Spark Logical Plan",description:"Prophecy deployment is flexible and supports multiple mechanisms",source:"@site/docs/spark-internals/plans/JobLogicalPlan.md",sourceDirName:"spark-internals/plans",slug:"/spark-internals/plans/spark-logical-plan",permalink:"/spark-internals/plans/spark-logical-plan",draft:!1,editUrl:"https://github.com/sparkplusplus/sparkplusplus.github.io/edit/main/docs/spark-internals/plans/JobLogicalPlan.md",tags:[{label:"overview",permalink:"/tags/overview"},{label:"spark-internals",permalink:"/tags/spark-internals"}],version:"current",sidebarPosition:2,frontMatter:{title:"Spark Logical Plan",sidebar_position:2,id:"spark-logical-plan",description:"Prophecy deployment is flexible and supports multiple mechanisms",tags:["overview","spark-internals"]},sidebar:"defaultSidebar",previous:{title:"Overview",permalink:"/spark-internals/overview"},next:{title:"Spark Physical Plan",permalink:"/spark-internals/plans/spark-physical-plan"}},l={},d=[{value:"An example of general logical plan",id:"an-example-of-general-logical-plan",level:3},{value:"Logical Plan",id:"logical-plan",level:2},{value:"1. How to produce RDD? What RDD should be produced?",id:"1-how-to-produce-rdd-what-rdd-should-be-produced",level:3},{value:"2. How to build RDD relationship?",id:"2-how-to-build-rdd-relationship",level:3},{value:"3. Illustration of typical dependencies and their computation",id:"3-illustration-of-typical-dependencies-and-their-computation",level:3},{value:"Primitive transformation()",id:"primitive-transformation",level:2},{value:"Discussion",id:"discussion",level:2}],s={toc:d},m="wrapper";function k(e){let{components:t,...r}=e;return(0,i.kt)(m,(0,a.Z)({},s,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h3",{id:"an-example-of-general-logical-plan"},"An example of general logical plan"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"deploy",src:n(8387).Z,width:"3570",height:"1620"})),(0,i.kt)("p",null,"The picture above illustrates a general job logical plan which takes 4 steps to get the final result:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Construct the initial ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," from any source.(in-memory data, local file, HDFS, HBase, etc). (Note that ",(0,i.kt)("inlineCode",{parentName:"p"},"parallelize()")," is equivalent to ",(0,i.kt)("inlineCode",{parentName:"p"},"createRDD()")," mentioned in the previous chapter)")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"A series of ",(0,i.kt)("em",{parentName:"p"},"transformation operations")," on ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD"),", denoted as ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()"),", each ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()")," produces one or more ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD[T]"),"s, where ",(0,i.kt)("inlineCode",{parentName:"p"},"T")," can be any type in Scala."),(0,i.kt)("blockquote",{parentName:"li"},(0,i.kt)("pre",{parentName:"blockquote"},(0,i.kt)("code",{parentName:"pre"},"Need to mention that, for key-value pair `RDD[(K, V)]`, it will be handy if `K` is a basic type, like `Int`, `Double`, `String`, etc. It can not be collection type, like `Array`, `List`, etc, since it is hard to define `partition` function on collections\n")))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("em",{parentName:"p"},"Action operation"),", denoted as ",(0,i.kt)("inlineCode",{parentName:"p"},"action()")," is called on final ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD"),", then each partition produces a result")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"These results will be sent to the driver, then ",(0,i.kt)("inlineCode",{parentName:"p"},"f(List[Result])")," will be computed as the final result to client side, for example, ",(0,i.kt)("inlineCode",{parentName:"p"},"count()")," takes two steps, ",(0,i.kt)("inlineCode",{parentName:"p"},"action()")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"sum()"),"."))),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"RDD can be cached into memory or on hard disk, by calling ",(0,i.kt)("inlineCode",{parentName:"p"},"cache()"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"persist()")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"checkpoint()"),". The number of partitions is usually set by user. Partition relationship between 2 RDDs can be not 1 to 1. In the picture above, we can see not only 1 to 1 relationship, but also many to many ones.")),(0,i.kt)("h2",{id:"logical-plan"},"Logical Plan"),(0,i.kt)("p",null,"When writing your spark code, you might also have a dependency diagram in you mind (like the one above). However, the reality is that some more ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD"),"s will be produced."),(0,i.kt)("p",null,"In order to make this more clear, we will talk about :"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"How to produce RDD ? What kind of RDD should be produced ?"),(0,i.kt)("li",{parentName:"ul"},"How to build dependency relationship between RDDs ?")),(0,i.kt)("h3",{id:"1-how-to-produce-rdd-what-rdd-should-be-produced"},"1. How to produce RDD? What RDD should be produced?"),(0,i.kt)("p",null,"A ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()")," usually returns a new ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD"),", but some ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()"),"s which are more complicated and contain several sub-",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()")," produce multiple ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD"),"s. That's why the number of RDDs is, in fact, more than we thought."),(0,i.kt)("p",null,"Logical plan is essentially a ",(0,i.kt)("em",{parentName:"p"},"computing chain"),". Every ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," has a ",(0,i.kt)("inlineCode",{parentName:"p"},"compute()")," method which takes the input records of the previous ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," or data source, then performs ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()"),", finally outputs computed records."),(0,i.kt)("p",null,"What ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," to be produced depends on the computing logic. Let's talk about some typical ",(0,i.kt)("a",{parentName:"p",href:"http://spark.apache.org/docs/latest/programming-guide.html#transformations"},"transformation()")," and the RDDs they produce."),(0,i.kt)("p",null,"We can learn about the meaning of each ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()")," on Spark site. More details are listed in the following table, where ",(0,i.kt)("inlineCode",{parentName:"p"},"iterator(split)")," means ",(0,i.kt)("em",{parentName:"p"},"for each record in partition"),". There are some blanks in the table, because they are complex ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()")," producing multiple RDDs, they will be illustrated soon after."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"left"},"Transformation"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Generated RDDs"),(0,i.kt)("th",{parentName:"tr",align:"left"},"Compute()"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"map"),"(func)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"MappedRDD"),(0,i.kt)("td",{parentName:"tr",align:"left"},"iterator(split).map(f)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"filter"),"(func)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"FilteredRDD"),(0,i.kt)("td",{parentName:"tr",align:"left"},"iterator(split).filter(f)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"flatMap"),"(func)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"FlatMappedRDD"),(0,i.kt)("td",{parentName:"tr",align:"left"},"iterator(split).flatMap(f)")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"mapPartitions"),"(func)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"MapPartitionsRDD"),(0,i.kt)("td",{parentName:"tr",align:"left"},"f(iterator(split))")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"mapPartitionsWithIndex"),"(func)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"MapPartitionsRDD"),(0,i.kt)("td",{parentName:"tr",align:"left"},"f(split.index, iterator(split))")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"sample"),"(withReplacement, fraction, seed)"),(0,i.kt)("td",{parentName:"tr",align:"left"},"PartitionwiseSampledRDD"),(0,i.kt)("td",{parentName:"tr",align:"left"},"PoissonSampler.sample(iterator(split))  BernoulliSampler.sample(iterator(split))")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"pipe"),"(command, ","[envVars]",")"),(0,i.kt)("td",{parentName:"tr",align:"left"},"PipedRDD"),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"union"),"(otherDataset)"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"intersection"),"(otherDataset)"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"distinct"),"(","[numTasks]","))"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"groupByKey"),"(","[numTasks]",")"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"reduceByKey"),"(func, ","[numTasks]",")"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"sortByKey"),"(","[ascending]",", ","[numTasks]",")"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"join"),"(otherDataset, ","[numTasks]",")"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"cogroup"),"(otherDataset, ","[numTasks]",")"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"cartesian"),"(otherDataset)"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"coalesce"),"(numPartitions)"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"left"},(0,i.kt)("strong",{parentName:"td"},"repartition"),"(numPartitions)"),(0,i.kt)("td",{parentName:"tr",align:"left"}),(0,i.kt)("td",{parentName:"tr",align:"left"})))),(0,i.kt)("h3",{id:"2-how-to-build-rdd-relationship"},"2. How to build RDD relationship?"),(0,i.kt)("p",null,"We need to figure out the following things:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"RDD dependencies. ",(0,i.kt)("inlineCode",{parentName:"li"},"RDD x")," depends on one parent RDD or several parent RDDs?"),(0,i.kt)("li",{parentName:"ul"},"How many partitions are there in ",(0,i.kt)("inlineCode",{parentName:"li"},"RDD x"),"?"),(0,i.kt)("li",{parentName:"ul"},"What's the relationship between the partitions of ",(0,i.kt)("inlineCode",{parentName:"li"},"RDD x")," and those of its parent RDD(s)? One partition depends one or several partition of parent RDD?")),(0,i.kt)("p",null,"The first question is trivial, such as ",(0,i.kt)("inlineCode",{parentName:"p"},"x = rdda.transformation(rddb)"),", e.g., ",(0,i.kt)("inlineCode",{parentName:"p"},"val x = a.join(b)")," means that ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD x")," depends both ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD a")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD b")),(0,i.kt)("p",null,"For the second question, as mentioned before, the number of partitions is defined by user, by default, it takes ",(0,i.kt)("inlineCode",{parentName:"p"},"max(numPartitions[parent RDD 1], ..., numPartitions[parent RDD n])")),(0,i.kt)("p",null,"The third one is a little bit complex, we need to consider the meaning of a ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()"),". Different ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()"),"s have different dependency. For example, ",(0,i.kt)("inlineCode",{parentName:"p"},"map()")," is 1:1, while ",(0,i.kt)("inlineCode",{parentName:"p"},"groupByKey()")," produces a ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffledRDD")," in which each partition depends on all partitions in its parent RDD. Besides this, some ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()")," can be more complex."),(0,i.kt)("p",null,"In spark, there are 2 kinds of dependencies which are defined in terms of parent RDD's partition:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"NarrowDependency (OneToOneDependency, RangeDependency)"),(0,i.kt)("blockquote",{parentName:"li"},(0,i.kt)("pre",{parentName:"blockquote"},(0,i.kt)("code",{parentName:"pre"},"each partition of the child RDD depends on a small number of partitions of the parent RDD, i.e. a child partition depends the **entire** parent partition. (**full dependency**)\n")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"ShuffleDependency (or wide dependency, mentioned in Matei's paper)"),(0,i.kt)("blockquote",{parentName:"li"},(0,i.kt)("pre",{parentName:"blockquote"},(0,i.kt)("code",{parentName:"pre"},"multiple child partitions depends on a parent partition, i.e. each child partition depends **a part of** the parent partition. (**partial dependency**)\n"))))),(0,i.kt)("p",null,"For example, ",(0,i.kt)("inlineCode",{parentName:"p"},"map")," leads to a narrow dependency, while ",(0,i.kt)("inlineCode",{parentName:"p"},"join")," leads to to wide\ndependencies (unless the two parents are hash-partitioned)."),(0,i.kt)("p",null,"On the other hand, each child partition can depend on one partition in parent RDD, or some partitions in parent RDD."),(0,i.kt)("p",null,"Note that:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"For ",(0,i.kt)("inlineCode",{parentName:"li"},"NarrowDependency"),", whether a child partition needs one or multiple parent partition depends on ",(0,i.kt)("inlineCode",{parentName:"li"},"getParents(partition i)")," function in child RDD. (More details later)"),(0,i.kt)("li",{parentName:"ul"},"ShuffleDependency is like shuffle dependency  in MapReduce\uff08mapper partitions its output, then each reducer will fetch all the needed output partitions via http.fetch)")),(0,i.kt)("p",null,"The two dependencies are illustrated in the following picture."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Dependency",src:n(1180).Z,width:"527",height:"506"})),(0,i.kt)("p",null,"According to the definition, the two on the first row are ",(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency")," and the last one is ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffleDependency"),"."),(0,i.kt)("p",null,"Need to mention that the left one on the second row is a very rare case between two RDD. It is a ",(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency")," (N:N) whose logical plan is like ShuffleDependency, but it is a full dependency. It can be created by some tricks. We will not talk about this, because, more strictly, ",(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency")," essentially means ",(0,i.kt)("strong",{parentName:"p"},"each partition of the parent RDD is used by at most one partition of the child RDD"),". Some typical RDD dependencies will be talked about soon."),(0,i.kt)("p",null,"To conclude, partition dependencies are listed as below"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"NarrowDependency (black arrow)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"RangeDependency -> only for UnionRDD"),(0,i.kt)("li",{parentName:"ul"},"OneToOneDependency (1:1) -> e.g. map, filter"),(0,i.kt)("li",{parentName:"ul"},"NarrowDependency (N:1) -> e.g. join co-partitioned"),(0,i.kt)("li",{parentName:"ul"},"NarrowDependency (N:N) -> rare case"))),(0,i.kt)("li",{parentName:"ul"},"ShuffleDependency (red arrow)")),(0,i.kt)("p",null,"Note that, in the rest of this chapter, ",(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency")," will be represented by black arrow and ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffleDependency")," are red ones."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffleDependency")," are needed for physical plan which will be talked about in the next chapter."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"How to compute records in RDD x")),(0,i.kt)("p",null,"An ",(0,i.kt)("inlineCode",{parentName:"p"},"OneToOneDependency")," case is shown in the picture below. Although it is a 1 to 1 relationship between two partitions, it doesn't mean that records are read and computed one by one."),(0,i.kt)("p",null,"The difference between the two patterns on the right side is similar to the following code snippets."),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Dependency",src:n(5273).Z,width:"2708",height:"1245"})),(0,i.kt)("p",null,"code1 of iter.f()"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-java"},"int[] array = {1, 2, 3, 4, 5}\nfor(int i = 0; i < array.length; i++)\n    f(array[i])\n")),(0,i.kt)("p",null,"code2 of f(iter)"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-java"},"int[] array = {1, 2, 3, 4, 5}\nf(array)\n")),(0,i.kt)("h3",{id:"3-illustration-of-typical-dependencies-and-their-computation"},"3. Illustration of typical dependencies and their computation"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"1) union(otherRDD)")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"union",src:n(8680).Z,width:"248",height:"434"})),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"union()")," simply combines two RDDs together. It never changes  data of a partition. ",(0,i.kt)("inlineCode",{parentName:"p"},"RangeDependency"),"(1:1) retains the borders of original RDDs in order to make it easy to revisit the partitions from RDD produced by ",(0,i.kt)("inlineCode",{parentName:"p"},"union()")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"2) groupByKey(numPartitions)")," ","[changed in 1.3]"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"groupByKey",src:n(3734).Z,width:"504",height:"545"})),(0,i.kt)("p",null,"We have talked about ",(0,i.kt)("inlineCode",{parentName:"p"},"groupByKey"),"'s dependency before, now we make it more clear."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"groupByKey()")," combines records with the same key by shuffle. The ",(0,i.kt)("inlineCode",{parentName:"p"},"compute()")," function in ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffledRDD")," fetches necessary data for its partitions, then take ",(0,i.kt)("inlineCode",{parentName:"p"},"mapPartition()")," operation (like ",(0,i.kt)("inlineCode",{parentName:"p"},"OneToOneDependency"),"), ",(0,i.kt)("inlineCode",{parentName:"p"},"MapPartitionsRDD")," will be produced by ",(0,i.kt)("inlineCode",{parentName:"p"},"aggregate()"),". Finally, ",(0,i.kt)("inlineCode",{parentName:"p"},"ArrayBuffer")," type in the value is casted to ",(0,i.kt)("inlineCode",{parentName:"p"},"Iterable")),(0,i.kt)("blockquote",null,(0,i.kt)("pre",{parentName:"blockquote"},(0,i.kt)("code",{parentName:"pre"},"`groupByKey()` has no map side combine, because map side combine does not reduce the amount of data shuffled and requires all map side data be inserted into a hash table, leading to more objects in the old gen.\n"))),(0,i.kt)("blockquote",null,(0,i.kt)("pre",{parentName:"blockquote"},(0,i.kt)("code",{parentName:"pre"},"`ArrayBuffer` is essentially `a CompactBuffer` which is an append-only buffer similar to ArrayBuffer, but more memory-efficient for small buffers.\n"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"2) reduceyByKey(func, numPartitions)")," ","[changed in 1.3]"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"reduceyByKey",src:n(3597).Z,width:"600",height:"563"})),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"reduceByKey()")," is similar to ",(0,i.kt)("inlineCode",{parentName:"p"},"MapReduce"),". The data flow is equivalent. ",(0,i.kt)("inlineCode",{parentName:"p"},"redcuceByKey")," enables map side combine by default, which is carried out by ",(0,i.kt)("inlineCode",{parentName:"p"},"mapPartitions")," before shuffle and results in ",(0,i.kt)("inlineCode",{parentName:"p"},"MapPartitionsRDD"),". After shuffle, ",(0,i.kt)("inlineCode",{parentName:"p"},"aggregate + mapPartitions")," is applied to ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffledRDD"),". Again, we get a ",(0,i.kt)("inlineCode",{parentName:"p"},"MapPartitionsRDD")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"3) distinct(numPartitions)")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"distinct",src:n(6804).Z,width:"3266",height:"2408"})),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"distinct()")," aims to deduplicate RDD records. Since duplicated records can be found in different partitions, shuffle is needed to deduplicate records by using ",(0,i.kt)("inlineCode",{parentName:"p"},"aggregate()"),". However, shuffle need ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD[(K, V)]"),". If the original records have only keys, e.g. ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD[Int]"),", then it should be completed as ",(0,i.kt)("inlineCode",{parentName:"p"},"<K, null>")," by ",(0,i.kt)("inlineCode",{parentName:"p"},"map()")," (",(0,i.kt)("inlineCode",{parentName:"p"},"MappedRDD"),"). After that, ",(0,i.kt)("inlineCode",{parentName:"p"},"reduceByKey()")," is used to do some shuffle (mapSideCombine->reduce->MapPartitionsRDD). Finally, only key is taken from <K, null> by ",(0,i.kt)("inlineCode",{parentName:"p"},"map()"),"(",(0,i.kt)("inlineCode",{parentName:"p"},"MappedRDD"),"). ",(0,i.kt)("inlineCode",{parentName:"p"},"ReduceByKey()")," RDDs are colored in blue"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"4) cogroup(otherRDD, numPartitions)")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"cogroup",src:n(4796).Z,width:"1622",height:"1785"})),(0,i.kt)("p",null,"Different from ",(0,i.kt)("inlineCode",{parentName:"p"},"groupByKey()"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"cogroup()")," aggregates 2 or more RDDs. ",(0,i.kt)("strong",{parentName:"p"},"What's the relationship between GoGroupedRDD and (RDD a, RDD b)? ShuffleDependency or OneToOneDependency\uff1f")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"number of partition"),(0,i.kt)("p",{parentName:"li"},"The # of partition in ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD")," is defined by user, it has nothing to do with ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD a")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD b"),". However, if #partition of ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD")," is different from the one of ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD a/b"),", then it is not an ",(0,i.kt)("inlineCode",{parentName:"p"},"OneToOneDependency"),".")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"type of partitioner"),(0,i.kt)("p",{parentName:"li"},"The ",(0,i.kt)("inlineCode",{parentName:"p"},"partitioner")," defined by user (",(0,i.kt)("inlineCode",{parentName:"p"},"HashPartitioner")," by default) for ",(0,i.kt)("inlineCode",{parentName:"p"},"cogroup()")," decides where to put the its results. Even if ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD a/b")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD")," have the same # of partition, while their partitioner are different, it can not be ",(0,i.kt)("inlineCode",{parentName:"p"},"OneToOneDependency"),". Let's take the examples in the picture above, ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD a")," is ",(0,i.kt)("inlineCode",{parentName:"p"},"RangePartitioner"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD b")," is ",(0,i.kt)("inlineCode",{parentName:"p"},"HashPartitioner"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD")," is ",(0,i.kt)("inlineCode",{parentName:"p"},"RangePartitioner")," with the same # partition as ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD a"),". Obviously, the records in each partition of ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD a")," can be directly sent to the corresponding partitions in ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD"),", but those in ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD b")," need to be divided in order to be shuffled into the right partitions of ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD"),"."))),(0,i.kt)("p",null,"To conclude, ",(0,i.kt)("inlineCode",{parentName:"p"},"OneToOneDependency")," occurs iff the partitioner type and #partitions of 2 RDDs and ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD")," are the same, otherwise, the dependency must be a ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffleDependency"),". More details can be found in ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD.getDependencies()"),"'s source code"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"How does spark deal with the fact that ",(0,i.kt)("inlineCode",{parentName:"strong"},"CoGroupedRDD"),"'s partition depends on multiple parent partitions?")),(0,i.kt)("p",null,"Firstly, ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD")," put all needed ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," into ",(0,i.kt)("inlineCode",{parentName:"p"},"rdds: Array[RDD]")),(0,i.kt)("p",null,"Then,"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Foreach rdd = rdds(i):\n    if CoGroupedRDD and rdds(i) are OneToOneDependency\n        Dependecy[i] = new OneToOneDependency(rdd)\n    else\n        Dependecy[i] = new ShuffleDependency(rdd)\n")),(0,i.kt)("p",null,"Finally, it returns ",(0,i.kt)("inlineCode",{parentName:"p"},"deps: Array[Dependency]")," which is an array of ",(0,i.kt)("inlineCode",{parentName:"p"},"Dependency")," corresponding to  each parent RDD."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"Dependency.getParents(partition id)")," returns ",(0,i.kt)("inlineCode",{parentName:"p"},"partitions: List[Int]")," which are the necessary parent partitions of the specified partition (",(0,i.kt)("inlineCode",{parentName:"p"},"partition id"),") with respect to the given ",(0,i.kt)("inlineCode",{parentName:"p"},"Dependency")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"getPartitions()")," tells how many partitions are in ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," and how each partition is serialized."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"5) intersection(otherRDD)")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"intersection",src:n(6539).Z,width:"3329",height:"3533"})),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"intersection()")," aims to extract all the common elements from  ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD a and b"),". ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD[T]")," is mapped into ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD[(T, null)]"),", where ",(0,i.kt)("inlineCode",{parentName:"p"},"T")," can not be any collections, then ",(0,i.kt)("inlineCode",{parentName:"p"},"a.cogroup(b)")," (colored in blue). ",(0,i.kt)("inlineCode",{parentName:"p"},"filter()")," only keeps records where neither of ",(0,i.kt)("inlineCode",{parentName:"p"},"[iter(groupA()), iter(groupB())]")," is empty (",(0,i.kt)("inlineCode",{parentName:"p"},"FilteredRDD"),"). Finally, only ",(0,i.kt)("inlineCode",{parentName:"p"},"keys()")," are kept (",(0,i.kt)("inlineCode",{parentName:"p"},"MappedRDD"),")"),(0,i.kt)("p",null,"6) ",(0,i.kt)("strong",{parentName:"p"},"join(otherRDD, numPartitions)")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"join",src:n(3700).Z,width:"3304",height:"5529"})),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"join()")," takes two ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD[(K, V)]"),", like ",(0,i.kt)("inlineCode",{parentName:"p"},"join")," in SQL. Similar to ",(0,i.kt)("inlineCode",{parentName:"p"},"intersection()"),", it does ",(0,i.kt)("inlineCode",{parentName:"p"},"cogroup()")," first and results in a ",(0,i.kt)("inlineCode",{parentName:"p"},"MappedValuesRDD")," whose type is ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD[(K, (Iterable[V1], Iterable[V2]))]"),", then compute the Cartesian product between the two ",(0,i.kt)("inlineCode",{parentName:"p"},"Iterable"),", finally ",(0,i.kt)("inlineCode",{parentName:"p"},"flatMap()")," is called."),(0,i.kt)("p",null,"Here are two examples, in the first one, ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD 1")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD 2")," use ",(0,i.kt)("inlineCode",{parentName:"p"},"RangePartitioner"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD")," takes ",(0,i.kt)("inlineCode",{parentName:"p"},"HashPartitioner")," which is different from ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD 1/2"),", so it's a ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffleDependency"),". In the second one, ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD 1")," is initially partitioned on key by ",(0,i.kt)("inlineCode",{parentName:"p"},"HashPartitioner")," and gets 3 partition which is the same as the one ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupedRDD")," takes, so it's a ",(0,i.kt)("inlineCode",{parentName:"p"},"OneToOneDependency"),". Furthermore, if ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD 2")," is also initially divided by ",(0,i.kt)("inlineCode",{parentName:"p"},"HashPartitioner(3)"),", then there is no ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffleDependency"),". This kind of ",(0,i.kt)("inlineCode",{parentName:"p"},"join")," is called ",(0,i.kt)("inlineCode",{parentName:"p"},"hashjoin()")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"7) sortByKey(ascending, numPartitions)")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"sortByKey",src:n(6726).Z,width:"490",height:"563"})),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"sortByKey()")," sorts records of ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD[(K, V)]")," by key. ",(0,i.kt)("inlineCode",{parentName:"p"},"ascending")," is a self-explanatory boolean flag. It produces a ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffledRDD")," which takes a ",(0,i.kt)("inlineCode",{parentName:"p"},"rangePartitioner"),". The partitioner decides the border of each partition, e.g. the first partition takes records with keys from ",(0,i.kt)("inlineCode",{parentName:"p"},"char A")," to ",(0,i.kt)("inlineCode",{parentName:"p"},"char B"),", and the second takes those from ",(0,i.kt)("inlineCode",{parentName:"p"},"char C")," to ",(0,i.kt)("inlineCode",{parentName:"p"},"char D"),". Inside each partition, records are sorted by key. Finally, the records in ",(0,i.kt)("inlineCode",{parentName:"p"},"MapPartitionsRDD")," are in order."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("inlineCode",{parentName:"p"},"sortByKey()")," use ",(0,i.kt)("inlineCode",{parentName:"p"},"Array")," to store records of each partition, then sorts them.")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"8) cartesian(otherRDD)")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"cartesian",src:n(9535).Z,width:"3179",height:"1783"})),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"Cartesian()")," returns a Cartesian product of 2 ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD"),"s. The resulting ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," has ",(0,i.kt)("inlineCode",{parentName:"p"},"#partition(RDD a) x #partition(RDD b)")," partitions."),(0,i.kt)("p",null,"Need to pay attention to the dependency, each partition in ",(0,i.kt)("inlineCode",{parentName:"p"},"CartesianRDD")," depends 2 ",(0,i.kt)("strong",{parentName:"p"},"entire")," parent RDDs. They are all ",(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency"),"."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("inlineCode",{parentName:"p"},"CartesianRDD.getDependencies()")," returns ",(0,i.kt)("inlineCode",{parentName:"p"},"rdds: Array(RDD a, RDD b)"),". The ith partition of ",(0,i.kt)("inlineCode",{parentName:"p"},"CartesianRDD")," depends:"),(0,i.kt)("ul",{parentName:"blockquote"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"a.partitions(i / #partitionA)")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"b.partitions(i % #partitionB)")))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"9) coalesce(numPartitions, shuffle = false)")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Coalesce",src:n(7929).Z,width:"3291",height:"4883"})),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"coalesce()")," can reorganize partitions, e.g. decrease # of partitions from 5 to 3, or increase from 5 to 10. Need to notice that when ",(0,i.kt)("inlineCode",{parentName:"p"},"shuffle = false"),", we can not increase partitions, because that will force a shuffle while we don't want shuffle, which is nonsense."),(0,i.kt)("p",null,"To understand ",(0,i.kt)("inlineCode",{parentName:"p"},"coalesce()"),", we need to know ",(0,i.kt)("strong",{parentName:"p"},"the relationship between ",(0,i.kt)("inlineCode",{parentName:"strong"},"CoalescedRDD"),"'s partitions and its parent partitions")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("inlineCode",{parentName:"p"},"coalesce(shuffle = false)"),"\nAs shuffle is disabled, what we need to do is just to group certain parent partitions. In fact, there are many factors to take into consideration, e.g. # records in partition, locality ,balance, etc. Spark has a rather complicated algorithm to do with that. (we will not talk about that for the moment). For example, ",(0,i.kt)("inlineCode",{parentName:"p"},"a.coalesce(3, shuffle = false)")," is essentially a ",(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency")," of N:1.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre"},"`coalesce(shuffle = true)`\n")),(0,i.kt)("p",{parentName:"li"},"When shuffle is enabled, ",(0,i.kt)("inlineCode",{parentName:"p"},"coalesce")," simply divides all records of ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," in N parts, which can be done by the following trick (like round-robin algorithm):"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"for each partition, every record is assigned a key which is an increasing number."),(0,i.kt)("li",{parentName:"ul"},"hash(key) leads to a uniform records distribution on all different partitions.")),(0,i.kt)("p",{parentName:"li"},"In the second example, every element in ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD a")," is combined with a increasing key (on the left side of the pair). The key of the first element in a partition is equal to ",(0,i.kt)("inlineCode",{parentName:"p"},"(new Random(index)).nextInt(numPartitions)"),", where ",(0,i.kt)("inlineCode",{parentName:"p"},"index")," is the index of the partition and ",(0,i.kt)("inlineCode",{parentName:"p"},"numPartitions")," is the # of partitions in ",(0,i.kt)("inlineCode",{parentName:"p"},"CoalescedRDD"),". The following keys increase by 1. After shuffle, the records in ",(0,i.kt)("inlineCode",{parentName:"p"},"ShffledRDD")," are uniformly distributed. The relationship between ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffledRDD")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"CoalescedRDD")," is defined a complicated algorithm. In the end, keys are removed (",(0,i.kt)("inlineCode",{parentName:"p"},"MappedRDD"),")."))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"10) repartition(numPartitions)")),(0,i.kt)("p",null,"equivalent to coalesce(numPartitions, shuffle = true)"),(0,i.kt)("h2",{id:"primitive-transformation"},"Primitive transformation()"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"combineByKey()")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"So far, we have seen a lot of logic plans. It's true that some of them are very similar. The reason lies in their implementation.")),(0,i.kt)("p",null,"Knowing that the RDD on left side of ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffleDependency")," should be ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD[(K, V)]"),", while, on the right side, all records with the same key are aggregated, then different operation will be applied on these aggregated records."),(0,i.kt)("p",null,"In fact, many ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()"),", like ",(0,i.kt)("inlineCode",{parentName:"p"},"groupByKey()"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"reduceBykey()"),", executes ",(0,i.kt)("inlineCode",{parentName:"p"},"aggregate()")," while doing logical computation. So ",(0,i.kt)("strong",{parentName:"p"},"the similarity is that ",(0,i.kt)("inlineCode",{parentName:"strong"},"aggregate()")," and ",(0,i.kt)("inlineCode",{parentName:"strong"},"compute()")," are executed in the same time.")," Spark uses ",(0,i.kt)("inlineCode",{parentName:"p"},"combineByKey()")," to implement ",(0,i.kt)("inlineCode",{parentName:"p"},"aggregate() + compute()")," operation."),(0,i.kt)("p",null,"Here is the definition of ",(0,i.kt)("inlineCode",{parentName:"p"},"combineByKey()")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"}," def combineByKey[C](createCombiner: V => C,\n      mergeValue: (C, V) => C,\n      mergeCombiners: (C, C) => C,\n      partitioner: Partitioner,\n      mapSideCombine: Boolean = true,\n      serializer: Serializer = null): RDD[(K, C)]\n")),(0,i.kt)("p",null,"There are three important parameters to talk about:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"createCombiner"),", which turns a V into a C (e.g., creates a one-element list)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"mergeValue"),", to merge a V into a C (e.g., adds it to the end of a list)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"mergeCombiners"),", to combine two C's into a single one.")),(0,i.kt)("p",null,"Details:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"When some (K, V) pair records are being pushed to ",(0,i.kt)("inlineCode",{parentName:"li"},"combineByKey()"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"createCombiner")," takes the first record to initialize a combiner of type ",(0,i.kt)("inlineCode",{parentName:"li"},"C")," (e.g. C = V)."),(0,i.kt)("li",{parentName:"ul"},"From then on, ",(0,i.kt)("inlineCode",{parentName:"li"},"mergeValue")," takes every incoming record, ",(0,i.kt)("inlineCode",{parentName:"li"},"mergeValue(combiner, record.value)"),", to update the combiner. Let's take ",(0,i.kt)("inlineCode",{parentName:"li"},"sum")," as an example, ",(0,i.kt)("inlineCode",{parentName:"li"},"combiner = combiner + recoder.value"),". In the end, all concerned records are merged into the combiner"),(0,i.kt)("li",{parentName:"ul"},"If there is another set of records with the same key as the pairs above. ",(0,i.kt)("inlineCode",{parentName:"li"},"combineByKey()")," will produce another ",(0,i.kt)("inlineCode",{parentName:"li"},"combiner'"),". In the last step, the final result is equal to ",(0,i.kt)("inlineCode",{parentName:"li"},"mergeCombiners(combiner, combiner')"),".")),(0,i.kt)("h2",{id:"discussion"},"Discussion"),(0,i.kt)("p",null,"So far, we have discussed how to produce job's logical plan as well as the complex dependency and computation behind spark"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"tranformation()")," decides what kind of RDDs will be produced. Some ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()")," are reused by other operations (e.g. ",(0,i.kt)("inlineCode",{parentName:"p"},"cogroup"),")"),(0,i.kt)("p",null,"The dependency of a ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," depends on how ",(0,i.kt)("inlineCode",{parentName:"p"},"transformation()")," produces corresponding RDD. e.g. ",(0,i.kt)("inlineCode",{parentName:"p"},"CoGroupdRDD")," depends on all ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD"),"s used for ",(0,i.kt)("inlineCode",{parentName:"p"},"cogroup()")),(0,i.kt)("p",null,"The relationship of ",(0,i.kt)("inlineCode",{parentName:"p"},"RDD")," partitions are ",(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"ShuffleDependency"),". The former is ",(0,i.kt)("strong",{parentName:"p"},"full dependency")," and the latter is ",(0,i.kt)("strong",{parentName:"p"},"partial dependency"),". ",(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency")," can be represented in many cases, a dependency is a ",(0,i.kt)("inlineCode",{parentName:"p"},"NarrowDependency")," iff #partition and partitioner type are the same."),(0,i.kt)("p",null,"In terms of data flow, ",(0,i.kt)("inlineCode",{parentName:"p"},"MapReduce")," is equivalent to ",(0,i.kt)("inlineCode",{parentName:"p"},"map() + reduceByKey()"),". Technically, the ",(0,i.kt)("inlineCode",{parentName:"p"},"reduce")," of ",(0,i.kt)("inlineCode",{parentName:"p"},"MapReduce")," would be more powerful than ",(0,i.kt)("inlineCode",{parentName:"p"},"reduceByKey()"),". These details will be talked about in chapter ",(0,i.kt)("strong",{parentName:"p"},"Shuffle details"),"."))}k.isMDXComponent=!0},9535:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Cartesian-903663465d2599b8ffbf7e0d0f55480b.png"},7929:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Coalesce-ad1e0d3bffa29c851de06a19ca5b07cb.png"},1180:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/Dependency-01238505defacd59e7ac8f33472bb887.png"},8387:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/GeneralLogicalPlan-34665c4e974f4869580bb15857b9133f.png"},5273:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/OneToOneDependency-b63fc5f5c976196e38610bab2b99a95e.png"},4796:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/cogroup-c6c2252dba13a75e9ed42f94a7fdc44c.png"},6804:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/distinct-3117e6436c2a14d1b16b36817251a97d.png"},3734:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/groupByKey-979a3cac7117108dba30c05445ab3b12.png"},6539:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/intersection-55e2a82054299816f364802950140d63.png"},3700:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/join-a1d574d68f1020fdd8476b607bab9bcb.png"},3597:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/reduceByKey-baadf0a05f5e85c3f8bed7b664840589.png"},6726:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/sortByKey-dd1abe102f712cbccd931556ea6a64a7.png"},8680:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/union-87f32da054b157ad576a0342bc977d08.png"}}]);