"use strict";(self.webpackChunkdocs_4=self.webpackChunkdocs_4||[]).push([[802],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>b});var r=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,o=function(e,t){if(null==e)return{};var a,r,o={},n=Object.keys(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=r.createContext({}),d=function(e){var t=r.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},c=function(e){var t=d(e.components);return r.createElement(l.Provider,{value:t},e.children)},p="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var a=e.components,o=e.mdxType,n=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(a),m=o,b=p["".concat(l,".").concat(m)]||p[m]||h[m]||n;return a?r.createElement(b,i(i({ref:t},c),{},{components:a})):r.createElement(b,i({ref:t},c))}));function b(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var n=a.length,i=new Array(n);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:o,i[1]=s;for(var d=2;d<n;d++)i[d]=a[d];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}m.displayName="MDXCreateElement"},8774:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>n,metadata:()=>s,toc:()=>d});var r=a(7462),o=(a(7294),a(3905));const n={title:"Broadcast",sidebar_position:7,id:"broadcast",description:"Prophecy deployment is flexible and supports multiple mechanisms",tags:["overview","spark-internals"]},i=void 0,s={unversionedId:"spark-internals/broadcast",id:"spark-internals/broadcast",title:"Broadcast",description:"Prophecy deployment is flexible and supports multiple mechanisms",source:"@site/docs/spark-internals/Broadcast.md",sourceDirName:"spark-internals",slug:"/spark-internals/broadcast",permalink:"/spark-internals/broadcast",draft:!1,editUrl:"https://github.com/sparkplusplus/sparkplusplus.github.io/edit/main/docs/spark-internals/Broadcast.md",tags:[{label:"overview",permalink:"/tags/overview"},{label:"spark-internals",permalink:"/tags/spark-internals"}],version:"current",sidebarPosition:7,frontMatter:{title:"Broadcast",sidebar_position:7,id:"broadcast",description:"Prophecy deployment is flexible and supports multiple mechanisms",tags:["overview","spark-internals"]},sidebar:"defaultSidebar",previous:{title:"Cache And Checkpoint",permalink:"/spark-internals/cache-and-checkpoint"}},l={},d=[{value:"Why read-only?",id:"why-read-only",level:3},{value:"Why broadcast to nodes but not tasks?",id:"why-broadcast-to-nodes-but-not-tasks",level:3},{value:"How to use broadcast?",id:"how-to-use-broadcast",level:3},{value:"How is broadcast implemented?",id:"how-is-broadcast-implemented",level:3},{value:"Distribute metadata of the broadcast variable",id:"distribute-metadata-of-the-broadcast-variable",level:4},{value:"HttpBroadcast",id:"httpbroadcast",level:4},{value:"TorrentBroadcast",id:"torrentbroadcast",level:4},{value:"Driver",id:"driver",level:4},{value:"Executor",id:"executor",level:4},{value:"Discussion",id:"discussion",level:2}],c={toc:d},p="wrapper";function h(e){let{components:t,...n}=e;return(0,o.kt)(p,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"As its name implies, broadcast means sending data from one node to all other nodes in the cluster. It's useful in many situations, for example we have a table in the driver, and other nodes need it as a lookup table. With broadcast we can send this table to all nodes and tasks will be able to do local lookups. Actually, it is challenging to implement a broadcast mechanism that is reliable and efficient. Spark's documentation says:"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Broadcast variables allow the programmer to keep a ",(0,o.kt)("strong",{parentName:"p"},"read-only")," variable cached on each ",(0,o.kt)("strong",{parentName:"p"},"machine")," rather than shipping a copy of it with ",(0,o.kt)("strong",{parentName:"p"},"tasks"),". They can be used, for example, to give every node a copy of a ",(0,o.kt)("strong",{parentName:"p"},"large input dataset")," in an efficient manner. Spark also attempts to distribute broadcast variables using ",(0,o.kt)("strong",{parentName:"p"},"efficient")," broadcast algorithms to reduce communication cost.")),(0,o.kt)("h3",{id:"why-read-only"},"Why read-only?"),(0,o.kt)("p",null,"This is a consistency problem. If a broadcasted variable can be mutated, once it's modified in some node, should we also update the copies on other nodes? If multiple nodes have updated their copies, how do we determine an order to synchronize these independent updates? There's also fault-tolerance problem coming in. To avoid all these tricky problems with data consistency, Spark only supports read-only broadcast variables."),(0,o.kt)("h3",{id:"why-broadcast-to-nodes-but-not-tasks"},"Why broadcast to nodes but not tasks?"),(0,o.kt)("p",null,"Each task runs inside a thread, and all tasks in a process belong to the same Spark application. So a single read-only copy in each node (executor) can be shared by all tasks."),(0,o.kt)("h3",{id:"how-to-use-broadcast"},"How to use broadcast?"),(0,o.kt)("p",null,"An example of a driver program:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-scala"},"val data = List(1, 2, 3, 4, 5, 6)\nval bdata = sc.broadcast(data)\n\nval rdd = sc.parallelize(1 to 6, 2)\nval observedSizes = rdd.map(_ => bdata.value.size)\n")),(0,o.kt)("p",null,"Driver uses ",(0,o.kt)("inlineCode",{parentName:"p"},"sc.broadcast()")," to declare the data to broadcast. The type of ",(0,o.kt)("inlineCode",{parentName:"p"},"bdata")," is ",(0,o.kt)("inlineCode",{parentName:"p"},"Broadcast"),". ",(0,o.kt)("inlineCode",{parentName:"p"},"rdd.transformation(func)")," uses ",(0,o.kt)("inlineCode",{parentName:"p"},"bdata")," directly inside its function like a local variable."),(0,o.kt)("h3",{id:"how-is-broadcast-implemented"},"How is broadcast implemented?"),(0,o.kt)("p",null,"The implementation behind the broadcast feature is quite interesting."),(0,o.kt)("h4",{id:"distribute-metadata-of-the-broadcast-variable"},"Distribute metadata of the broadcast variable"),(0,o.kt)("p",null,"Driver creates a local directory to store the data to be broadcasted and launches a ",(0,o.kt)("inlineCode",{parentName:"p"},"HttpServer")," with access to the directory. The data is actually written into the directory when the broadcast is called (",(0,o.kt)("inlineCode",{parentName:"p"},"val bdata = sc.broadcast(data)"),"). At the same time, the data is also written into driver's ",(0,o.kt)("inlineCode",{parentName:"p"},"blockManger")," with a ",(0,o.kt)("inlineCode",{parentName:"p"},"StorageLevel")," memory + disk. Block manager allocates a ",(0,o.kt)("inlineCode",{parentName:"p"},"blockId")," (of type ",(0,o.kt)("inlineCode",{parentName:"p"},"BroadcastBlockId"),") for the data. When a transformation function uses the broadcasted variable, the driver's ",(0,o.kt)("inlineCode",{parentName:"p"},"submitTask()")," will serialize its metadata and send it along with the serialized function to all nodes. Akka system impose message size limits so we can not use it to broadcast the actual data."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Why driver put the data in both local disk directory and block manager? The copy on the local disk directory is for the HttpServer, and the copy in block manager is to facilitate the usage of this data inside the driver program.")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Then when the real data is broadcasted?")," When an executor deserializes the task it has received, it also gets the broadcast variable's metadata, in the form of a ",(0,o.kt)("inlineCode",{parentName:"p"},"Broadcast")," object. It then calls the ",(0,o.kt)("inlineCode",{parentName:"p"},"readObject()")," method of the metadata object (",(0,o.kt)("inlineCode",{parentName:"p"},"bdata")," variable). This method will first check the local block manager to see if there's already a local copy. If not, the data will be fetched from the driver. Once the data is fetched, it's stored in the local block manager for subsequent uses."),(0,o.kt)("p",null,"Spark has actually 2 different implementations of the data fetching."),(0,o.kt)("h4",{id:"httpbroadcast"},"HttpBroadcast"),(0,o.kt)("p",null,"This method fetches data through an http connection between the executor and the driver."),(0,o.kt)("p",null,"Driver creates a ",(0,o.kt)("inlineCode",{parentName:"p"},"HttpBroadcast")," object and it's this object's job to store the broadcast data into the driver's block manager. In the same time, as we described earlier, the data is written on the local disk, for example in a directory named ",(0,o.kt)("inlineCode",{parentName:"p"},"/var/folders/87/grpn1_fn4xq5wdqmxk31v0l00000gp/T/spark-6233b09c-3c72-4a4d-832b-6c0791d0eb9c/broadcast_0"),"."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Driver and executor instantiate a ",(0,o.kt)("inlineCode",{parentName:"p"},"broadcastManger")," object during initialization. The local directory is created by ",(0,o.kt)("inlineCode",{parentName:"p"},"HttpBroadcast.initialize()")," method. This method also launches the http server.")),(0,o.kt)("p",null,"The actual fetching is just data transmission between two nodes with an http connection."),(0,o.kt)("p",null,"The problem of ",(0,o.kt)("inlineCode",{parentName:"p"},"HttpBroadcast")," is network bandwidth bottleneck in the driver node since it sends data to all other worker nodes at the same time."),(0,o.kt)("h4",{id:"torrentbroadcast"},"TorrentBroadcast"),(0,o.kt)("p",null,"To solve the driver network bottleneck problem in ",(0,o.kt)("inlineCode",{parentName:"p"},"HttpBroadcast"),", Spark introduced a new broadcast implementation called ",(0,o.kt)("inlineCode",{parentName:"p"},"TorrentBroadcast")," which is inspired by BitTorrent. The basic concept of this method is to cut the broadcast data in blocks, and executors who have already fetched data blocks can themselves be the data source."),(0,o.kt)("p",null,"Unlike the data transfer in ",(0,o.kt)("inlineCode",{parentName:"p"},"HttpBroadcast"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"TorrentBroadcast")," uses ",(0,o.kt)("inlineCode",{parentName:"p"},"blockManager.getRemote() => NIO ConnectionManager")," to do the job. The actual sending and receiving process is quite similar with the cached rdd that we've talked about in the last chapter (check last diagram in ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/JerryLead/SparkInternals/blob/master/markdown/6-CacheAndCheckpoint.md"},"CacheAndCheckpoint"),"."),(0,o.kt)("p",null,"Let's see some details in ",(0,o.kt)("inlineCode",{parentName:"p"},"TorrentBroadcast"),":"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"TorrentBroadcast",src:a(3525).Z,width:"602",height:"276"})),(0,o.kt)("h4",{id:"driver"},"Driver"),(0,o.kt)("p",null,"The driver serializes the data into ",(0,o.kt)("inlineCode",{parentName:"p"},"ByteArray")," and then cut it into ",(0,o.kt)("inlineCode",{parentName:"p"},"BLOCK_SIZE")," (defined by ",(0,o.kt)("inlineCode",{parentName:"p"},"spark.broadcast.blockSize = 4MB"),") blocks. After the cut the original ",(0,o.kt)("inlineCode",{parentName:"p"},"ByteArray")," will be collected but there's a temporary moment we have 2 copies of the data in memory."),(0,o.kt)("p",null,"After the cut, information about the blocks (called metadata) is stored in driver's block manager with a storage level at memory + disk. At this time, driver's ",(0,o.kt)("inlineCode",{parentName:"p"},"blockManagerMaster")," will also be informed that the metadata is successfully stored. ",(0,o.kt)("strong",{parentName:"p"},"This is an important step because blockManagerMaster can be accessed by all executors, meaning that block metadata has now become global data of the cluster.")),(0,o.kt)("p",null,"Driver then finishes its work by physically storing the data blocks under its block manager."),(0,o.kt)("h4",{id:"executor"},"Executor"),(0,o.kt)("p",null,"Upon receiving a serialized task, an executor deserializes it first. The deserialization also includes the broadcast metadata, whose type is ",(0,o.kt)("inlineCode",{parentName:"p"},"TorrentBroadcast"),". Then its ",(0,o.kt)("inlineCode",{parentName:"p"},"TorrentBroadcast.readObject()")," method is called. Similar to the general steps illustrated above, the local block manager will be checked first to see if some data blocks have already been fetched. If not, executor will ask driver's ",(0,o.kt)("inlineCode",{parentName:"p"},"blockManagerMaster")," for the data block's metadata. Then the BitTorrent process starts to fetch the data blocks."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"BitTorrent process:")," an ",(0,o.kt)("inlineCode",{parentName:"p"},"arrayOfBlocks = new Array[TorrentBlock](totalBlocks)")," is allocated locally to store the fetched data. ",(0,o.kt)("inlineCode",{parentName:"p"},"TorrentBlock")," is a wrapper over a data block. The actual fetching order is randomized. For example if there's 5 blocks to fetch in total, the fetching order may be 3-1-2-4-5. Then the executor starts to fetch the data block one by one: local ",(0,o.kt)("inlineCode",{parentName:"p"},"blockManager")," => local ",(0,o.kt)("inlineCode",{parentName:"p"},"connectionManager")," => driver/other executor's blockManager => data. ",(0,o.kt)("strong",{parentName:"p"},"Each fetched block is stored under local block manager and driver's ",(0,o.kt)("inlineCode",{parentName:"strong"},"blockManagerMaster")," is informed that this block has been successfully fetched.")," As you'll guess, this is an important step because now all other nodes in the cluster know that there's a new data source for this block. If another node starts to fetch the same block, it'll randomly choose one data source. With more and more blocks being fetched, we'll have more data sources and the whole broadcast will be accelerated. There's a good illustration about BitTorrent on ",(0,o.kt)("a",{parentName:"p",href:"http://zh.wikipedia.org/wiki/BitTorrent_(%E5%8D%8F%E8%AE%AE)"},"wikipedia"),"."),(0,o.kt)("p",null,"When all the data blocks are fetched locally, a big ",(0,o.kt)("inlineCode",{parentName:"p"},"Array[Byte]")," will be allocated to reconstitute the original broadcast data from blocks. Finally this array gets deserialized and is stored under local block manager. Notice that once we have the broadcast variable in local block manager, we can safely delete the fetched data blocks (which are also stored in local block manager)."),(0,o.kt)("p",null,"One more question: what about broadcasting an RDD? In fact nothing bad will happen. This RDD will be evaluated in each executor so that each node has a copy of its result."),(0,o.kt)("h2",{id:"discussion"},"Discussion"),(0,o.kt)("p",null,"Broadcasting shared variables is a very handy feature. In Hadoop we have the ",(0,o.kt)("inlineCode",{parentName:"p"},"DistributedCache")," and it's used in many situations. For example, parameters of ",(0,o.kt)("inlineCode",{parentName:"p"},"-libjars")," are sent to all nodes by using ",(0,o.kt)("inlineCode",{parentName:"p"},"DistributedCache"),". However in Hadoop broadcasted data needs to be uploaded to HDFS first and it has no mechanism to share data between tasks in the same node. Say if some node needs to process 4 mappers coming from the same job, then the broadcast variable will be stored 4 times in this node (one copy in each mapper's working directory). An advantage of this approach is that by using HDFS we won't have the bottleneck problem since HDFS does the job of cutting data into blocks and distributing them across the cluster."),(0,o.kt)("p",null,"For Spark, broadcast cares about sending data to all nodes as well as letting tasks of the same node share data. Spark's block manager solves the problem of sharing data between tasks in the same node. Storing shared data in local block manager with a storage level at memory + disk guarantees that all local tasks can access the shared data, in this way we avoid storing multiple copies. Spark has 2 broadcast implementations. The traditional ",(0,o.kt)("inlineCode",{parentName:"p"},"HttpBroadcast")," has the bottleneck problem around the driver node. ",(0,o.kt)("inlineCode",{parentName:"p"},"TorrentBroadcast")," solves this problem but it starts slower since it only accelerate the broadcast after some amount of blocks fetched by executors. Also in Spark, the reconstitution of original data from data blocks needs some extra memory space."),(0,o.kt)("p",null,"In fact Spark also tried an alternative called ",(0,o.kt)("inlineCode",{parentName:"p"},"TreeBroadcast"),". Interested reader can check the technical report: ",(0,o.kt)("a",{parentName:"p",href:"http://www.cs.berkeley.edu/~agearh/cs267.sp10/files/mosharaf-spark-bc-report-spring10.pdf"},"Performance and Scalability of Broadcast in Spark"),"."),(0,o.kt)("p",null,"In my opinion the broadcast feature can even be implemented by using multicast protocols. But multicast is UDP based, we'll need mechanisms on reliability in the application layer."))}h.isMDXComponent=!0},3525:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/TorrentBroadcast-e9b8c8c04c6c8421655c3e0b429f45d4.png"}}]);