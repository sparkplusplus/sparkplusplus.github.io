<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-spark-internals/shuffle-details">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Shuffle Details | SparkPlusPlus</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://sparkplusplus.github.io/spark-internals/shuffle-details"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Shuffle Details | SparkPlusPlus"><meta data-rh="true" name="description" content="Prophecy deployment is flexible and supports multiple mechanisms"><meta data-rh="true" property="og:description" content="Prophecy deployment is flexible and supports multiple mechanisms"><link data-rh="true" rel="icon" href="/img/favicon.png"><link data-rh="true" rel="canonical" href="https://sparkplusplus.github.io/spark-internals/shuffle-details"><link data-rh="true" rel="alternate" href="https://sparkplusplus.github.io/spark-internals/shuffle-details" hreflang="en"><link data-rh="true" rel="alternate" href="https://sparkplusplus.github.io/spark-internals/shuffle-details" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://P8WC4Z2QXP-dsn.algolia.net" crossorigin="anonymous"><link rel="search" type="application/opensearchdescription+xml" title="SparkPlusPlus" href="/opensearch.xml">


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><link rel="stylesheet" href="/assets/css/styles.b22f5b3f.css">
<link rel="preload" href="/assets/js/runtime~main.3b17f140.js" as="script">
<link rel="preload" href="/assets/js/main.3185e24c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/Logo.png" alt="SparkPlusPlus Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/Logo.png" alt="SparkPlusPlus Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate"></b></a><a href="https://github.com/sparkplusplus" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Home</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Spark Internals</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/spark-internals/">Spark Internals</a><button aria-label="Toggle the collapsible sidebar category &#x27;Spark Internals&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/spark-internals/overview">Overview</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/spark-internals/plans/spark-logical-plan">Spark Planning</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/spark-internals/shuffle-details">Shuffle Details</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/spark-internals/architecture">Architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/spark-internals/cache-and-checkpoint">Cache And Checkpoint</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/spark-internals/broadcast">Broadcast</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/spark-internals/"><span itemprop="name">Spark Internals</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Shuffle Details</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Shuffle Process</h1><p>Previously we&#x27;ve discussed Spark&#x27;s physical plan and its execution details. But one thing is left untouched: <strong>how data gets through a <code>ShuffleDependency</code> to the next stage?</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="shuffle-comparison-between-hadoop-and-spark">Shuffle Comparison between Hadoop and Spark<a href="#shuffle-comparison-between-hadoop-and-spark" class="hash-link" aria-label="Direct link to Shuffle Comparison between Hadoop and Spark" title="Direct link to Shuffle Comparison between Hadoop and Spark">​</a></h2><p>There&#x27;re some differences and also similarities between the shuffle process in Hadoop and in Spark:</p><p><strong>From a high-level point of view, they are similar.</strong> They both partition the mapper&#x27;s (or <code>ShuffleMapTask</code> in Spark) output and send each partition to its corresponding reducer (in Spark, it could be a <code>ShuffleMapTask</code> in the next stage, or a <code>ResultTask</code>). The reducer buffers the data in memory, shuffles and aggregates the data, and applies the <code>reduce()</code> logic once the data is aggregated.</p><p><strong>From a low-level point of view, there&#x27;re quite a few differences.</strong> The shuffle in Hadoop is sort-based since the records must be sorted before <code>combine()</code> and <code>reduce()</code>. The sort can be done by an external sort algorithm thus allowing <code>combine()</code> or <code>reduce()</code> to tackle very large datasets. Currently in Spark the default shuffle process is hash-based. Usually it uses a <code>HashMap</code> to aggregate the shuffle data and no sort is applied. If the data needs to be sorted, user has to call <code>sortByKey()</code> explicitly. In Spark 1.1, we can set the configuration <code>spark.shuffle.manager</code> to <code>sort</code> to enable sort-based shuffle. In Spark 1.2, the default shuffle process will be sort-based.</p><p><strong>Implementation-wise, there&#x27;re also differences.</strong> As we know, there are obvious steps in a Hadoop workflow: <code>map()</code>, <code>spill</code>, <code>merge</code>, <code>shuffle</code>, <code>sort</code> and <code>reduce()</code>. Each step has a predefined responsibility and it fits the procedural programming model well. However in Spark, there&#x27;re no such fixed steps, instead we have stages and a series of transformations. So operations like <code>spill</code>, <code>merge</code> and <code>aggregate</code> need to be somehow included in the transformations.</p><p>If we name the mapper side process of partitioning and persisting data &quot;shuffle write&quot;, and the reducer side reading and aggregating data &quot;shuffle read&quot;. Then the problem becomes: <strong>How to integrate shuffle write and shuffle read logic in Spark&#x27;s logical or physical plan? How to implement shuffle write and shuffle read efficiently?</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="shuffle-write">Shuffle Write<a href="#shuffle-write" class="hash-link" aria-label="Direct link to Shuffle Write" title="Direct link to Shuffle Write">​</a></h2><p>Shuffle write is a relatively simple task if a sorted output is not required. It partitions and persists the data. The persistance of data here has two advantages: reducing heap pressure and enhancing fault-tolerance.</p><p>Its implementation is simple: add the shuffle write logic at the end of <code>ShuffleMapStage</code> (in which there&#x27;s a <code>ShuffleMapTask</code>). Each output record of the final RDD in this stage is partitioned and persisted, as shown in the following diagram:</p><p><img loading="lazy" alt="shuffle-write-no-consolidation" src="/assets/images/shuffle-write-no-consolidation-df8e2a45cd3d74f6137a289da02ee871.png" width="2164" height="1264" class="img_ev3q"></p><p>In the diagram there&#x27;re 4 <code>ShuffleMapTask</code>s to execute in the same worker node with 2 cores. The task result (records of the final RDD in the stage) is written on the local disk (data persistence). Each task has <code>R</code> buffers, <code>R</code> equals the number of reducers (the number if tasks in the next stage). The buffers are called buckets in Spark. By default the size of each bucket is 32KB (100KB before Spark 1.1) and is configurable by <code>spark.shuffle.file.buffer.kb</code> .</p><blockquote><p>In fact bucket is a general concept in Spark that represents the location of the partitioned output of a <code>ShuffleMapTask</code>. Here for simplicity a bucket is referred to an in-memory buffer.</p></blockquote><p><code>ShuffleMapTask</code> employs the pipelining techinque to compute the result records of the final RDD. Each record is sent to the bucket of its corresponding partition, which is determined by <code>partitioner.partition(record.getKey())</code>. The content of these buckets is written continuously to local disk files called <code>ShuffleBlockFile</code>, or <code>FileSegment</code> for short. Reducers will fetch their <code>FileSegment</code> in shuffle read phase.</p><p>An implementation like this is very simple, but has some issues:</p><ol><li><strong>We may produce too many <code>FileSegment</code>.</strong> Each <code>ShuffleMapTask</code> produces <code>R</code>(number of reducers) <code>FileSegment</code>, so <code>M</code> <code>ShuffleMapTask</code> will produce <code>M * R</code> files. For big datasets we could have big <code>M</code> and <code>R</code>, as a result there may be lots of intermediate data files.</li><li><strong>Buffers could take a lot of space.</strong> On a worker node, we could have <code>R * M</code> buckets for each core available to Spark. Spark will reuse the buffer space after a <code>ShuffleMapTask</code> but there could still be <code>R * cores</code> buckets in memory. On a node with 8 cores processing a 1000-reducer job, buckets will take up 256MB (<code>R * cores * 32KB</code>).</li></ol><p>Currently, there&#x27;s no good solution to the second problem. We need to write buffers anyway and if they&#x27;re too small there will be impact on IO speed. For the first problem, we have a file consolidation solution already implemented in Spark. Let&#x27;s check it out:</p><p><img loading="lazy" alt="shuffle-write-consolidation" src="/assets/images/shuffle-write-consolidation-5093d2e3c7affa1e7bc4cafa1036b8e7.png" width="1752" height="1358" class="img_ev3q"></p><p>It&#x27;s clear that from the above diagram, consecutive <code>ShuffleMapTask</code>s running on the same core share a shuffle file. Each task appends its output data, <code>ShuffleBlock</code> i&#x27;, after the output data of the previous task, <code>ShuffleBlock</code> i. A <code>ShuffleBlock</code> is called a <code>FileSegment</code>. In this way, reducers in the next stage can just fetch the whole file and we reduce the number of files needed in each worker node to <code>cores * R</code>. File consolidation feature can be activated by setting <code>spark.shuffle.consolidateFiles</code> to true.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="shuffle-read">Shuffle Read<a href="#shuffle-read" class="hash-link" aria-label="Direct link to Shuffle Read" title="Direct link to Shuffle Read">​</a></h2><p>Let&#x27;s check a physical plan of <code>reduceBykey</code>, which contains <code>ShuffleDependency</code>:</p><p><img loading="lazy" alt="reduceByKey" src="/assets/images/reduceByKeyStage-80d81b3cca5c75832e26936ec40e4caa.png" width="1733" height="1247" class="img_ev3q"></p><p>Intuitively, we need to fetch the data of <code>MapPartitionRDD</code> to be able to evaluate <code>ShuffleRDD</code>. Then come the problems:</p><ul><li>When to fetch? Fetch for each <code>ShuffleMapTask</code> or fetch only once after all <code>ShuffleMapTask</code>s are done?</li><li>Fetch and process the records at the same time or fetch and then process?</li><li>Where to store the fetched data?</li><li>How do the tasks of the next stage know the location of the fetched data?</li></ul><p>Solutions in Spark:</p><ul><li><p><strong>When to fetch?</strong> Wait after all <code>ShuffleMapTask</code>s end and then fetch. We know that a stage will be executed only after its parent stages are executed, so it&#x27;s intuitive that the fetch operation begins after all <code>ShuffleMapTask</code>s in the previous stage are done. The fetched <code>FileSegments</code> have to be buffered in memory, so we can&#x27;t fetch too much before the buffer content is written to disk. Spark limits this buffer size by <code>spark.reducer.maxMbInFlight</code>, here we name it <code>softBuffer</code>. It has default size 48MB. A <code>softBuffer</code> usually contains multiple fetched <code>FileSegments</code>. But sometimes one single segment can fill up the buffer.</p></li><li><p><strong>Fetch and process the records at the same time or fetch and then process?</strong> Fetch and process the records at the same time. In MapReduce, the shuffle stage fetches the data and then applies <code>combine()</code> logic at the same time. However in MapReduce the reducer input data needs to be sorted, so the <code>reduce()</code> logic is applied after the shuffle-sort process. Since Spark does not require a sorted order for the reducer input data, we don&#x27;t need to wait until all the data gets fetched to start processing. <strong>Then how Spark implements this shuffle and processing?</strong> In fact Spark utilizes data structures like HashMap to do the job. Each \&lt;Key, Value<!-- -->&gt;<!-- --> pair from the shuffle process is inserted into a HashMap. If the <code>Key</code> is already present, then the pair is aggregated by <code>func(hashMap.get(Key), Value)</code>. In the above WordCount example, the <code>func</code> is <code>hashMap.get(Key) + Value</code>, and its result is updated in the HashMap. This <code>func</code> has a similar role to <code>reduce()</code> in Hadoop, but they differ in details. We illustrate the difference by the following code snippet:</p><div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">// MapReduce</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">reduce</span><span class="token punctuation" style="color:#393A34">(</span><span class="token class-name">K</span><span class="token plain"> key</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token class-name">Iterable</span><span class="token generics punctuation" style="color:#393A34">&lt;</span><span class="token generics class-name">V</span><span class="token generics punctuation" style="color:#393A34">&gt;</span><span class="token plain"> values</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    result </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">process</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">key</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> values</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">// Spark</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">reduce</span><span class="token punctuation" style="color:#393A34">(</span><span class="token class-name">K</span><span class="token plain"> key</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token class-name">Iterable</span><span class="token generics punctuation" style="color:#393A34">&lt;</span><span class="token generics class-name">V</span><span class="token generics punctuation" style="color:#393A34">&gt;</span><span class="token plain"> values</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    result </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">null</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token class-name">V</span><span class="token plain"> value </span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> values</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        result  </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">func</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">result</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> value</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">return</span><span class="token plain"> result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></li></ul><p>In Hadoop MapReduce, we can define any data structure we like in <code>process</code> function. It&#x27;s just a function that takes an <code>Iterable</code> as parameter. We can also choose to cache the <code>values</code> for further processing. In Spark, a <code>foldLeft</code> like technique is used to apply the <code>func</code>. For example, in Hadoop, it&#x27;s very easy to compute the average out of <code>values</code>: <code>sum(values) / values.length</code>. But it&#x27;s not the case in the Spark model. We&#x27;ll come back to this part later.</p><ul><li><p><strong>Where to store the fetched data?</strong> The fetched <code>FileSegment</code>s get buffered in <code>softBuffer</code>. Then the data is processed, and written to a configurable location. If <code>spark.shuffle.spill</code> is false, then the write location is only memory. A special data structure, <code>AppendOnlyMap</code>, is used to hold these processed data in memory. Otherwise, the processed data will be written to memory and disk, using <code>ExternalAppendOnlyMap</code>. This data structure can spill the sorted key-value pairs on disk when there isn&#x27;t enough memory available. <strong>A key problem in using both memory and disk is how to find a balance of the two.</strong> In Hadoop, by default 70% of the memory is reserved for shuffle data. Once 66% of this part of the memory is used, Hadoop starts the merge-combine-spill process. In Spark a similar strategy is used. We&#x27;ll talk about its details later in this chapter.</p></li><li><p><strong>How do the tasks of the next stage know the location of the fetched data?</strong> Recall that in the last chapter, there&#x27;s an important step: <code>ShuffleMapStage</code>, which will register its final RDD by calling <code>MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size)</code>. So during the shuffle process, reducers get the data location by querying <code>MapOutputTrackerMaster</code> in the driver process. When a <code>ShuffleMapTask</code> finishes, it will report the location of its <code>FileSegment</code> to <code>MapOutputTrackerMaster</code>.</p></li></ul><p>Now we have discussed the main ideas behind shuffle write and shuffle read as well as some implementation details. Let&#x27;s dive into some interesting details.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="shuffle-read-of-typical-transformations">Shuffle Read of Typical Transformations<a href="#shuffle-read-of-typical-transformations" class="hash-link" aria-label="Direct link to Shuffle Read of Typical Transformations" title="Direct link to Shuffle Read of Typical Transformations">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="reducebykeyfunc"><code>reduceByKey(func)</code><a href="#reducebykeyfunc" class="hash-link" aria-label="Direct link to reducebykeyfunc" title="Direct link to reducebykeyfunc">​</a></h3><p>We have briefly talked about the fetch and reduce process of <code>reduceByKey()</code>. Note that for an RDD, not all its data is present in the memory at a given time. The processing is always on a record basis. Processed record is rejected if possible. On a record level perspective, the <code>reduce()</code> logic can be shown as below:</p><p><img loading="lazy" alt="shuffle-reduce" src="/assets/images/reduceByKeyRecord-6e3b1d9208b53372e8d12d7362c90ce5.png" width="598" height="416" class="img_ev3q"></p><p>We can see that the fetched records are aggregated using a HashMap, and once all the records are aggregated, we will have the result. The <code>func</code> needs to be commutative.</p><p>A <code>mapPartitionsWithContext</code> operation is used to transform the <code>ShuffledRDD</code> to a <code>MapPartitionsRDD</code>.</p><p>To reduce network trafic between nodes, we could use map side <code>combine()</code> in Hadoop. It&#x27;s also feasible in Spark. All we need is to apply the <code>mapPartitionsWithContext</code> in the <code>ShuffleMapStage</code>. For example in <code>reduceByKey</code> , the transformation of <code>ParallelCollectionRDD</code> to <code>MapPartitionsRDD</code> is equivalent to a map side combine.</p><p><strong>Comparison between map()-&gt;reduce() in Hadoop and <code>reduceByKey</code> in Spark</strong></p><ul><li>map side: there&#x27;s no difference on the map side. For <code>combine()</code> logic, Hadoop imposes a sort before <code>combine()</code>. Spark applies the <code>combine()</code> logic by using a hash map.</li><li>reduce side: Shuffle process in Hadoop will fetch the data until a certain amount, then applies <code>combine()</code> logic, then merge sort the data to feed the <code>reduce()</code> function. In Spark fetch and reduce is done at the same time (in a hash map), so the reduce function needs to be commutative.</li></ul><p><strong>Comparison in terms of memory usage</strong></p><ul><li>map side: Hadoop needs a big, circular buffer to hold and sort the <code>map()</code> output data. But <code>combine()</code> does not need extra space. Spark needs a hash map to do <code>combine()</code>. And persisting records to local disk needs buffers (buckets).</li><li>reduce side: Hadoop needs some memory space to store shuffled data. <code>combine()</code> and <code>reduce()</code> require no extra space since their input is sorted and can be grouped and then aggregated. In Spark, a <code>softBuffer</code> is needed for fetching. A hash map is used for storing the result of <code>combine()</code> and <code>reduce()</code>, if only memory is used in processing data. However, part of the data can be stored on disk if configured to use both memory and disk.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="groupbykeynumpartitions"><code>groupByKey(numPartitions)</code><a href="#groupbykeynumpartitions" class="hash-link" aria-label="Direct link to groupbykeynumpartitions" title="Direct link to groupbykeynumpartitions">​</a></h3><p><img loading="lazy" alt="ShuffleGroupByKey" src="/assets/images/ShuffleGroupByKey-56d9b95d0d8977f6452c5aae07c9282e.png" width="522" height="284" class="img_ev3q"></p><p>The process is similar to that of <code>reduceByKey()</code>. The <code>func</code> becomes <code>result = result ++ record.value</code>. This means that each key&#x27;s values are grouped together without further aggregation.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="distinctnumpartitions"><code>distinct(numPartitions)</code><a href="#distinctnumpartitions" class="hash-link" aria-label="Direct link to distinctnumpartitions" title="Direct link to distinctnumpartitions">​</a></h3><p><img loading="lazy" alt="ShuffleDistinct" src="/assets/images/ShuffleDistinct-1f360f91fc85645a0183f38595cf3fc3.png" width="1633" height="610" class="img_ev3q"></p><p>Similar to <code>reduceByKey()</code>. The <code>func</code> is <code>result = result == null ? record.value : result</code>. This means that we check the existence of the record in the <code>HashMap</code>. If it exists, reject the record, otherwise insert it into the map. Like <code>reduceByKey()</code>, there&#x27;s map side <code>combine()</code>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cogroupotherrdd-numpartitions"><code>cogroup(otherRDD, numPartitions)</code><a href="#cogroupotherrdd-numpartitions" class="hash-link" aria-label="Direct link to cogroupotherrdd-numpartitions" title="Direct link to cogroupotherrdd-numpartitions">​</a></h3><p><img loading="lazy" alt="ShuffleCoGroup" src="/assets/images/ShuffleCoGroup-d292dfea7e761e20a93db029031acdd2.png" width="535" height="419" class="img_ev3q"></p><p>There could be 0, 1 or multiple <code>ShuffleDependency</code> for a <code>CoGroupedRDD</code>. But in the shuffle process we don&#x27;t create a hash map for each shuffle dependency, but one hash map for all of them. Different from <code>reduceByKey</code>, the hash map is constructed in RDD&#x27;s <code>compute()</code> rather than in <code>mapPartitionsWithContext()</code>.</p><p>A task of this RDD&#x27;s execution will allocate an <code>Array[ArrayBuffer]</code>. This array contains the same number of empty <code>ArrayBuffer</code>s as the number of input RDDs. So in the example we have 2 <code>ArrayBuffers</code> in each task. When a key-value pair comes from RDD A, we add it to the first <code>ArrayBuffer</code>. If a key-value pair comes from RDD B, then it goes to the second <code>ArrayBuffer</code>. Finally a <code>mapValues()</code> operation transforms the values into the correct type: <code>(ArrayBuffer, ArrayBuffer)</code> =&gt; <code>(Iterable[V], Iterable[W])</code>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="intersectionotherrdd-and-joinotherrdd-numpartitions"><code>intersection(otherRDD)</code> and <code>join(otherRDD, numPartitions)</code><a href="#intersectionotherrdd-and-joinotherrdd-numpartitions" class="hash-link" aria-label="Direct link to intersectionotherrdd-and-joinotherrdd-numpartitions" title="Direct link to intersectionotherrdd-and-joinotherrdd-numpartitions">​</a></h3><p><img loading="lazy" alt="intersection" src="/assets/images/ShuffleIntersection-ac7cce7075bf9c580b3a710293fcc0cc.png" width="1660" height="852" class="img_ev3q"></p><p><img loading="lazy" alt="join" src="/assets/images/ShuffleJoin-95b56d52ad5cfd95e3511d3419c78bbd.png" width="1652" height="927" class="img_ev3q"></p><p>This two operations both use <code>cogroup</code>, so their shuffle process is identical to <code>cogroup</code>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sortbykeyascending-numpartition">sortByKey(ascending, numPartition)<a href="#sortbykeyascending-numpartition" class="hash-link" aria-label="Direct link to sortByKey(ascending, numPartition)" title="Direct link to sortByKey(ascending, numPartition)">​</a></h3><p><img loading="lazy" alt="sortByKey" src="/assets/images/ShuffleSortByKey-3ddfdb9c94b2aa4723c64d391fc624ec.png" width="490" height="284" class="img_ev3q"></p><p>The processing logic of <code>sortByKey()</code> is a little different from <code>reduceByKey()</code> as it does not use a <code>HashMap</code> to handle incoming fetched records. Instead, all key-value pairs are range partitioned. The records of the same partition is sorted by key.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="coalescenumpartitions-shuffle--true"><code>coalesce(numPartitions, shuffle = true)</code><a href="#coalescenumpartitions-shuffle--true" class="hash-link" aria-label="Direct link to coalescenumpartitions-shuffle--true" title="Direct link to coalescenumpartitions-shuffle--true">​</a></h3><p><img loading="lazy" alt="Coalesce" src="/assets/images/ShuffleCoalesce-201159fefbf6bb73036a0fc88af9cd9f.png" width="1585" height="791" class="img_ev3q"></p><p><code>coalesce()</code> would create a <code>ShuffleDependency</code>, but it actually does not need to aggregate the fetched records, so no hash map is needed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hashmap-in-shuffle-read">HashMap in Shuffle Read<a href="#hashmap-in-shuffle-read" class="hash-link" aria-label="Direct link to HashMap in Shuffle Read" title="Direct link to HashMap in Shuffle Read">​</a></h2><p>So as we have seen, hash map is a frequently used data structure in Spark&#x27;s shuffle process. Spark has 2 versions of specialized hash map: in memory <code>AppendOnlyMap</code> and memory-disk hybrid <code>ExternalAppendOnlyMap</code>. Let&#x27;s look at some details of these two hash map implementations.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="appendonlymap"><code>AppendOnlyMap</code><a href="#appendonlymap" class="hash-link" aria-label="Direct link to appendonlymap" title="Direct link to appendonlymap">​</a></h3><p>The Spark documentation describes <code>AppendOnlyMap</code> as &quot;A simple open hash table optimized for the append-only use case, where keys are never removed, but the value for each key may be changed&quot;. Its implementation is simple: allocate a big array of <code>Object</code>, as the following diagram shows. Keys are stored in the blue sections, and values are in the white sections.</p><p><img loading="lazy" alt="AppendOnlyMap" src="/assets/images/appendonlymap-bc4a365084f857c31a6c55eb67924799.png" width="1447" height="656" class="img_ev3q"></p><p>When a <code>put(K, V)</code> is issued, we locate the slot in the array by <code>hash(K)</code>. <strong>If the position is already occupied, then quadratic probing technique is used to find the next slot.</strong>. For the example in the diagram, <code>K6</code>, a third probing has found an empty slot after <code>K4</code>, then the value is inserted after the key. When <code>get(K6)</code>, we use the same technique to find the slot, get <code>V6</code> from the next slot, compute a new value, then write it to the position of <code>V6</code>.</p><p>Iteration over the <code>AppendOnlyMap</code> is just a scan of the array.</p><p>If 70% of the allocated array is used, then it will grow twice as large. Keys will be rehashed and the positions re-organized.</p><p>There&#x27;s a <code>destructiveSortedIterator(): Iterator[(K, V)]</code> method in <code>AppendOnlyMap</code>. It returns sorted key-value pairs. It&#x27;s implemented like this: first compact all key-value pairs to the front of the array and make each key-value pair in a single slot. Then <code>Array.sort()</code> is called to sort the array. As its name indicates, this operation will destroy the structure.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="externalappendonlymap"><code>ExternalAppendOnlyMap</code><a href="#externalappendonlymap" class="hash-link" aria-label="Direct link to externalappendonlymap" title="Direct link to externalappendonlymap">​</a></h3><p><img loading="lazy" alt="AppendOnlyMap" src="/assets/images/ExternalAppendOnlyMap-c09c623252fde527e8ba396216a72939.png" width="1714" height="2816" class="img_ev3q"></p><p>Compared with <code>AppendOnlyMap</code>, the implementation of <code>ExternalAppendOnlyMap</code> is more sophisticated. Its concept is similar to the <code>shuffle-merge-combine-sort</code> process in Hadoop.</p><p><code>ExternalAppendOnlyMap</code> holds an <code>AppendOnlyMap</code>. Incoming key-value pairs are inserted into the <code>AppendOnlyMap</code>. <strong>When <code>AppendOnlyMap</code> is about to grow its size, we&#x27;ll check the available memory space. If there&#x27;s still enough space, the <code>AppendOnlyMap</code> doubles its size, otherwise all its key-value pairs are sorted and then spilled onto local disk (by using <code>destructiveSortedIterator()</code>).</strong> In the diagram, there&#x27;re 4 spills of this map. In each spill, a <code>spillMap</code> file will be generated and a new, empty <code>AppendOnlyMap</code> will be instantiated to receive incoming key-value pairs. In <code>ExternalAppendOnlyMap</code>, when a key-value pair is inserted, it gets aggregated only with the in memory part (the <code>AppendOnlyMap</code>). So it means when asked for the final result, a global merge-aggregate needs to be performed on all spilled maps and the in memory <code>AppendOnlyMap</code>.</p><p><strong>Global merge-aggregate runs as follows.</strong> Firstly the in memory part (<code>AppendOnlyMap</code>) is sorted to a <code>sortedMap</code>. Then <code>DestructiveSortedIterator</code> (for <code>sortedMap</code>) or <code>DiskMapIterator</code> (for on disk <code>spillMap</code>) will be used to read a part of the key-value pairs into a <code>StreamBuffer</code>. Then the <code>StreamBuffer</code> is inserted into a <code>mergeHeap</code>. In each <code>StreamBuffer</code>, all records have the same <code>hash(key)</code>. Suppose that in the example, we have <code>hash(K1) == hash(K2) == hash(K3) &lt; hash(K4) &lt; hash(K5)</code>. As a result, the first 3 records of the first spilled map are read into the same <code>StreamBuffer</code>. The merge is simple: get <code>StreamBuffer</code>s with the same key hash using a heap, then put them into an <code>ArrayBuffer[StreamBuffer]</code>(<code>mergedBuffers</code>) for merge. The first inserted <code>StreamBuffer</code> is called <code>minBuffer</code>, the key of its first key-value pair is <code>minKey</code>. One merge operation will aggregate all KV pairs with <code>minKey</code> in the <code>mergedBuffer</code> and then output the result. When a merge operation in <code>mergedBuffer</code> is over, remaining KV pairs will return to the <code>mergeHeap</code>, and empty <code>StreamBuffer</code> will be replaced by a new read from in-memory map or on-disk spill.</p><p>There&#x27;re still 3 points needed to be discussed:</p><ul><li><p>Available memory check. Hadoop allocates 70% of the memory space of a reducer for shuffle-sort. Similarly, Spark has <code>spark.shuffle.memoryFraction * spark.shuffle.safetyFraction</code> (defaults to 0.3 * 0.8) for <code>ExternalAppendOnlyMap</code>. <strong>It seems that Spark is more conservative. Moreover, this 24% of memory space is shared by all reducers in the same executor.</strong> An executor holds a <code>ShuffleMemoryMap: HashMap[threadId, occupiedMemory]</code> to monitor memory usage of all <code>ExternalAppendOnlyMap</code>s in each reducer. Before an <code>AppendOnlyMap</code> grows, the total memory usage after the growth will be computed using the information in <code>ShuffleMemoryrMap</code>, to see if there&#x27;s enough space. Also notice that the first 1000 records will not trigger the spill check.</p></li><li><p><code>AppendOnlyMap</code> size estimation. To know the size of an <code>AppendOnlyMap</code>, we can compute the size of every object referenced in the structure during each growth. But this takes too much time. Spark has an estimation algorithm with O(1) complexity. Its core concept is to see how the map size changes after the insertion and aggregation of a certain amount of records to estimate the structure size. Details are in <code>SizeTrackingAppendOnlyMap</code> and <code>SizeEstimator</code>.</p></li><li><p>Spill process. Like the shuffle write, Spark creates a buffer when spilling records to disk. Its size is <code>spark.shuffle.file.buffer.kb</code>, defaulting to 32KB. Since the serializer also allocates buffers to do its job, there&#x27;ll be problems when we try to spill lots of records at the same time. Spark limits the records number that can be spilled at the same time to <code>spark.shuffle.spill.batchSize</code>, with a default value of 10000.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="discussion">Discussion<a href="#discussion" class="hash-link" aria-label="Direct link to Discussion" title="Direct link to Discussion">​</a></h2><p>As we&#x27;ve seen in this chapter, Spark is way more flexible in the shuffle process compared to Hadoop&#x27;s fixed shuffle-combine-merge-reduce model. It&#x27;s possible in Spark to combine different shuffle strategies with different data structures to design an appropriate shuffle process based on the semantic of the actual transformation.</p><p>So far we&#x27;ve discussed the shuffle process in Spark without sorting as well as how this process gets integrated into the actual execution of the RDD chain. We&#x27;ve also talked about memory and disk issues and compared some details with Hadoop. In the next chapter we&#x27;ll try to describe job execution from an inter-process communication perspective. The shuffle data location problem will also be mentioned.</p><p>Plus to this chapter, thers&#x27;s the outstanding blog (in Chinese) by Jerry Shao, <a href="http://jerryshao.me/2014/01/04/spark-shuffle-detail-investigation/" target="_blank" rel="noopener noreferrer">Deep Dive into Spark&#x27;s shuffle implementation</a>.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/overview">overview</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/tags/spark-internals">spark-internals</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/spark-internals/plans/spark-physical-plan"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Spark Physical Plan</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/spark-internals/architecture"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Architecture</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#shuffle-comparison-between-hadoop-and-spark" class="table-of-contents__link toc-highlight">Shuffle Comparison between Hadoop and Spark</a></li><li><a href="#shuffle-write" class="table-of-contents__link toc-highlight">Shuffle Write</a></li><li><a href="#shuffle-read" class="table-of-contents__link toc-highlight">Shuffle Read</a></li><li><a href="#shuffle-read-of-typical-transformations" class="table-of-contents__link toc-highlight">Shuffle Read of Typical Transformations</a><ul><li><a href="#reducebykeyfunc" class="table-of-contents__link toc-highlight"><code>reduceByKey(func)</code></a></li><li><a href="#groupbykeynumpartitions" class="table-of-contents__link toc-highlight"><code>groupByKey(numPartitions)</code></a></li><li><a href="#distinctnumpartitions" class="table-of-contents__link toc-highlight"><code>distinct(numPartitions)</code></a></li><li><a href="#cogroupotherrdd-numpartitions" class="table-of-contents__link toc-highlight"><code>cogroup(otherRDD, numPartitions)</code></a></li><li><a href="#intersectionotherrdd-and-joinotherrdd-numpartitions" class="table-of-contents__link toc-highlight"><code>intersection(otherRDD)</code> and <code>join(otherRDD, numPartitions)</code></a></li><li><a href="#sortbykeyascending-numpartition" class="table-of-contents__link toc-highlight">sortByKey(ascending, numPartition)</a></li><li><a href="#coalescenumpartitions-shuffle--true" class="table-of-contents__link toc-highlight"><code>coalesce(numPartitions, shuffle = true)</code></a></li></ul></li><li><a href="#hashmap-in-shuffle-read" class="table-of-contents__link toc-highlight">HashMap in Shuffle Read</a><ul><li><a href="#appendonlymap" class="table-of-contents__link toc-highlight"><code>AppendOnlyMap</code></a></li><li><a href="#externalappendonlymap" class="table-of-contents__link toc-highlight"><code>ExternalAppendOnlyMap</code></a></li></ul></li><li><a href="#discussion" class="table-of-contents__link toc-highlight">Discussion</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://join.slack.com/t/sparkplusplus/shared_invite/zt-2ceryo2v8-rb6m5M0Bqq02n_KKjP6CvQ" target="_blank" rel="noopener noreferrer" class="footer__link-item">Join our Slack<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/sparkplusplus/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">© 2024 SparkPlusPlus Inc All Rights Reserved target="_blank">Privacy Policy</div></div></div></footer></div>
<script src="/assets/js/runtime~main.3b17f140.js"></script>
<script src="/assets/js/main.3185e24c.js"></script>
</body>
</html>